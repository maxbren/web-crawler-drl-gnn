{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict, deque, defaultdict\n",
    "# from nervenet import NerveNet_GNN\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "# from utils import *\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab data and make Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_make_graph' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-14f191569de7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mG_whole\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_make_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/animals-D3-small-30K-nodes40-edges202-max10-minout2-minin3_w_features.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data_make_graph' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "G_whole, pages, node_feats, edges = load_data_make_graph('data/animals-D3-small-30K-nodes40-edges202-max10-minout2-minin3_w_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(G_whole, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, train_env, eval_env):\n",
    "        self.train_env = train_env\n",
    "        self.eval_env = eval_env\n",
    "        self.gnn = NerveNet_GNN(model_C['node_feat_size'], model_C['node_hidden_size'], \n",
    "                                model_C['message_size'], model_C['output_size'], \n",
    "                                goal_C['goal_size'], goal_C['goal_opt'], agent_C['critic_agg_weight'], \n",
    "                                device).to(device)\n",
    "        self.state = self.train_env.reset()\n",
    "        self.ep_step = 0\n",
    "        self.opt = torch.optim.Adam(self.gnn.parameters(), agent_C['learning_rate'])\n",
    "        self.gnn.eval()\n",
    "    \n",
    "    def _eval_episode(self, test_step):\n",
    "        state = self.eval_env.reset()\n",
    "        shortest_path_length = state[4]\n",
    "        ep_rew = 0\n",
    "        for step in range(episode_C['max_ep_steps']):\n",
    "            prediction = self.eval_env.propogate_multi(self.gnn, [state])\n",
    "            action = prediction['a'].cpu().numpy()[0]\n",
    "            state = deepcopy(state)\n",
    "            next_state, reward, done, achieved_goal = self.eval_env.step(action, step, state)\n",
    "            if achieved_goal: assert done\n",
    "            ep_rew += reward\n",
    "            test_step += 1\n",
    "            if done:\n",
    "                break\n",
    "            state = deepcopy(next_state)\n",
    "        return test_step, ep_rew, achieved_goal, shortest_path_length-1, step+1\n",
    "        \n",
    "    def eval_episodes(self):\n",
    "        test_step = 0\n",
    "        test_info = {}\n",
    "        test_info['all ep rew'] = []\n",
    "        test_info['max ep rew'] = float('-inf')\n",
    "        test_info['min ep rew'] = float('inf')\n",
    "        test_info['achieved goal'] = []\n",
    "        test_info['opt steps'] = []\n",
    "        test_info['steps taken'] = []\n",
    "        for ep in range(episode_C['eval_num_eps']):\n",
    "            test_step, ep_rew, achieved_goal, opt_steps, steps_taken = self._eval_episode(test_step)\n",
    "            test_info['all ep rew'].append(ep_rew)\n",
    "            test_info['max ep rew'] = max(test_info['max ep rew'], ep_rew)\n",
    "            test_info['min ep rew'] = min(test_info['min ep rew'], ep_rew)\n",
    "            test_info['achieved goal'].append(achieved_goal)\n",
    "            test_info['opt steps'].append(opt_steps)\n",
    "            test_info['steps taken'].append(steps_taken)\n",
    "        return (np.array(test_info['max ep rew']).mean(),\n",
    "                test_info['max ep rew'],\n",
    "                test_info['min ep rew'],\n",
    "                np.array(test_info['achieved goal']).sum() / ep,\n",
    "                np.array(test_info['opt steps']).mean(),\n",
    "                np.array(test_info['steps taken']).mean())\n",
    "        \n",
    "    def train_rollout(self, total_step):\n",
    "        storage = Storage(episode_C['rollout_length'])\n",
    "        state = Environment._copy_state(*self.state)\n",
    "        step_times = []\n",
    "        for rollout_step in range(episode_C['rollout_length']):\n",
    "            start_step_time = time.time()\n",
    "            prediction = self.train_env.propogate_multi(self.gnn, [state])\n",
    "            action = prediction['a'].cpu().numpy()[0]\n",
    "            next_state, reward, done, achieved_goal = self.train_env.step(action, self.ep_step, state)\n",
    "\n",
    "            self.ep_step += 1\n",
    "            if done:\n",
    "                self.ep_step = 0\n",
    "                \n",
    "            storage.add(prediction)\n",
    "            storage.add({'r': tensor(reward, device).unsqueeze(-1).unsqueeze(-1),\n",
    "                         'm': tensor(1 - done, device).unsqueeze(-1).unsqueeze(-1),\n",
    "                         's': state})\n",
    "\n",
    "            state = Environment._copy_state(*next_state)\n",
    "            \n",
    "            total_step += 1\n",
    "            \n",
    "            end_step_time = time.time()\n",
    "            step_times.append(end_step_time - start_step_time)\n",
    "            \n",
    "        self.state = Environment._copy_state(*state)\n",
    "\n",
    "        prediction = self.train_env.propogate_multi(self.gnn, [state])\n",
    "        storage.add(prediction)\n",
    "        storage.placeholder()\n",
    "        \n",
    "        advantages = tensor(np.zeros((1, 1)), device)\n",
    "        returns = prediction['v'].detach()\n",
    "        for i in reversed(range(episode_C['rollout_length'])):\n",
    "            # Disc. Return\n",
    "            returns = storage.r[i] + agent_C['discount'] * storage.m[i] * returns\n",
    "            # GAE\n",
    "            td_error = storage.r[i] + agent_C['discount'] * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "            advantages = advantages * agent_C['gae_tau'] * agent_C['discount'] * storage.m[i] + td_error\n",
    "            storage.adv[i] = advantages.detach()\n",
    "            storage.ret[i] = returns.detach()\n",
    "        \n",
    "#         print(returns.shape, td_error.shape, advantages.shape, storage.adv[-1].shape, storage.ret[-1].shape)\n",
    "        \n",
    "        actions, log_probs_old, returns, advantages = storage.cat(['a', 'log_pi_a', 'ret', 'adv'])\n",
    "        states = [storage.s[i] for i in range(storage.size)]\n",
    "        \n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        # Train\n",
    "        self.gnn.train()\n",
    "        batch_times = []\n",
    "        train_pred_times = []\n",
    "        for _ in range(agent_C['optimization_epochs']):\n",
    "            sampler = random_sample(np.arange(len(states)), agent_C['minibatch_size'])\n",
    "            for batch_indices in sampler:\n",
    "                start_batch_time = time.time()\n",
    "                \n",
    "                batch_indices_tensor = tensor(batch_indices, device).long()\n",
    "\n",
    "                # Important Node: these are tensors but dont have a grad\n",
    "                sampled_states = [states[i] for i in batch_indices]\n",
    "                sampled_actions = actions[batch_indices_tensor]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices_tensor]\n",
    "                sampled_returns = returns[batch_indices_tensor]\n",
    "                sampled_advantages = advantages[batch_indices_tensor]\n",
    "                \n",
    "                start_pred_time = time.time()\n",
    "                prediction = self.train_env.propogate_multi(self.gnn, sampled_states, sampled_actions)\n",
    "                end_pred_time = time.time()\n",
    "                train_pred_times.append(end_pred_time - start_pred_time)\n",
    "                \n",
    "                # Calc. Loss\n",
    "#                 self.gnn.train()\n",
    "                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - agent_C['ppo_ratio_clip'],\n",
    "                                          1.0 + agent_C['ppo_ratio_clip']) * sampled_advantages\n",
    "\n",
    "                # policy loss and value loss are scalars\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean() - agent_C['entropy_weight'] * prediction['ent'].mean()\n",
    "\n",
    "                value_loss = agent_C['value_loss_coef'] * (sampled_returns - prediction['v']).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                if agent_C['clip_grads']:\n",
    "                    nn.utils.clip_grad_norm_(self.gnn.parameters(), agent_C['gradient_clip'])\n",
    "#                 self.gnn.graph_grads()\n",
    "                self.opt.step()\n",
    "#                 self.gnn.eval()\n",
    "                end_batch_time = time.time()\n",
    "                batch_times.append(end_batch_time - start_batch_time)\n",
    "        self.gnn.eval()\n",
    "        return total_step, np.array(step_times).mean(), np.array(batch_times).mean(), np.array(train_pred_times).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Right now its just a collection of static methods, but I might wanna make it an actually useful class and\n",
    "# give it internal memory\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def step(selected_node_rel, ep_step, state):\n",
    "        state_copy = Environment._copy_state(*state)\n",
    "        G_curr, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors = state_copy[:]\n",
    "        \n",
    "        selected_node_abs = list(current_nodes.keys())[selected_node_rel]\n",
    "        achieved_goal = Environment.add_children(G_curr, selected_node_abs, goal_node, current_nodes)\n",
    "        # Done if goal found or end of ep\n",
    "        done = True if (ep_step == episode_C['max_ep_steps'] - 1) or achieved_goal else False\n",
    "        reward = Environment.reward_func(done, achieved_goal, shortest_path_length, ep_step+1)\n",
    "        predecessors = Environment.get_predecessors(G_curr, current_nodes)\n",
    "        \n",
    "        if not done:\n",
    "            next_state = Environment._copy_state(G_curr, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors)\n",
    "        else:\n",
    "            next_state = Environment.reset()\n",
    "        \n",
    "        return next_state, reward, done, achieved_goal\n",
    "    \n",
    "    # Deep copy everything before it goes into a state\n",
    "    @staticmethod\n",
    "    def _copy_state(G_curr, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors):\n",
    "        G_curr_copy = G_curr.copy()\n",
    "        current_nodes_copy = current_nodes.copy()\n",
    "        goal_feats_copy = goal_feats.copy()\n",
    "        predecessors_copy = deepcopy(predecessors)\n",
    "        \n",
    "        return (G_curr_copy, current_nodes_copy, goal_node, goal_feats_copy, shortest_path_length, predecessors_copy)\n",
    "    \n",
    "    # For now this is -1 per timestep +5 on terminal for reaching goal, -5 on terminal for not reaching goal\n",
    "    # And when it reaches the goal, give add (shortest_path_length - 1) - num actions taken (neg number)\n",
    "    @staticmethod\n",
    "    def reward_func(terminal, reach_goal, shortest_path_length, num_actions_taken):\n",
    "        rew = -1\n",
    "        if terminal:\n",
    "            if reach_goal:\n",
    "                assert num_actions_taken >= (shortest_path_length-1)\n",
    "                rew += other_C['reach_goal_rew']\n",
    "                rew += ((shortest_path_length-1) - num_actions_taken)  # optimal num actions - num actions taken\n",
    "            else:\n",
    "                rew += other_C['not_reach_goal_rew']\n",
    "        return rew\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        current_try = 0\n",
    "        while True:\n",
    "            current_try += 1\n",
    "    #         if current_try >= 50:\n",
    "    #              print('Current try for initialize ep is at: {}'.format(current_try))\n",
    "            init_node = random.randint(0, model_C['num_nodes']-1)\n",
    "            goal_node = random.randint(0, model_C['num_nodes']-1)\n",
    "            # restart if goal node is init node, or no path\n",
    "            if init_node == goal_node or not nx.has_path(G_whole, init_node, goal_node):\n",
    "                continue\n",
    "            # restart if shortest path is too long or too short\n",
    "            shortest_path_length = nx.shortest_path_length(G_whole, init_node, goal_node)\n",
    "            if shortest_path_length < episode_C['shortest_path_range_allowed_MIN'] or shortest_path_length > episode_C['shortest_path_range_allowed_MAX']:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # Get goal feats\n",
    "        goal_feats = node_feats[goal_node]\n",
    "        assert goal_feats.shape == (model_C['node_feat_size'],)\n",
    "        # Make init graph\n",
    "        G_init = nx.DiGraph()\n",
    "        G_init.add_node(init_node)\n",
    "        current_nodes = OrderedDict({init_node: 0})  # Init current nodes dict\n",
    "        got_goal = Environment.add_children(G_init, init_node, goal_node, current_nodes)\n",
    "        assert sorted(list(current_nodes.values())) == list(current_nodes.values())\n",
    "        assert not got_goal\n",
    "        predecessors = Environment.get_predecessors(G_init, current_nodes)\n",
    "        return (G_init, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_children(G_curr, node_indx, goal_node_indx, current_nodes):\n",
    "        achieved_goal = False\n",
    "        # Check the children of the node to see if they need to be added to the current graph\n",
    "        children = G_whole.successors(node_indx)\n",
    "        for child in children:\n",
    "            # Add child if not in G and check if goal\n",
    "            if child not in G_curr:\n",
    "                G_curr.add_node(child)\n",
    "                current_nodes.update({child: len(current_nodes)})\n",
    "                if child == goal_node_indx:\n",
    "                    achieved_goal = True\n",
    "            # If the edge doesnt exist add it\n",
    "            if not G_curr.has_edge(node_indx, child):\n",
    "                G_curr.add_edge(node_indx, child)\n",
    "        assert sorted(list(current_nodes.values())) == list(current_nodes.values())\n",
    "        return achieved_goal\n",
    "    \n",
    "    # current_nodes: ordereddict with keys as abs node indices, values as rel node indices (rel to the ordered dict)\n",
    "    @staticmethod\n",
    "    def get_predecessors(G_curr, current_nodes):\n",
    "        all_preds = []  # List of lists\n",
    "        for node in current_nodes.keys():\n",
    "            preds_abs = G_curr.predecessors(node)  # abs to all nodes, keys to the dict\n",
    "            preds_rel = [current_nodes[x] for x in preds_abs]\n",
    "            all_preds.append(preds_rel)\n",
    "        return all_preds  # Returns a list of lists with the values being tth rel node indices\n",
    "    \n",
    "    @staticmethod\n",
    "    def _unpack_states(states, node_feats_tensor):\n",
    "        num_nodes_all = []\n",
    "        goal_feats_all = []\n",
    "        predecessors_all = []\n",
    "        node_states_all = []\n",
    "        total_num_nodes = 0\n",
    "        for state in states:\n",
    "            G_curr, current_nodes, goal_node, goal_feats, shortest_path, predecessors = state[:]\n",
    "            num_nodes_all.append(len(current_nodes))\n",
    "            total_num_nodes += num_nodes_all[-1]\n",
    "            goal_feats_tensor = torch.tensor(goal_feats, device=device, requires_grad=True, dtype=torch.float)\n",
    "            goal_feats_all.append(goal_feats_tensor)\n",
    "            predecessors_all.append(predecessors)\n",
    "            node_states_all.append(node_feats_tensor[list(current_nodes.keys())])\n",
    "        node_states_all_tensor = torch.cat(node_states_all, dim=0)\n",
    "        goal_feats_all_tensor = torch.stack(goal_feats_all, dim=0)\n",
    "\n",
    "#         print(node_states_all_tensor.shape, goal_feats_all_tensor.shape, len(num_nodes_all), len(predecessors_all), total_num_nodes)\n",
    "#         print(node_states_all_tensor, goal_feats_all_tensor, num_nodes_all, predecessors_all)\n",
    "    \n",
    "        return node_states_all_tensor, goal_feats_all_tensor, num_nodes_all, predecessors_all, total_num_nodes\n",
    "    \n",
    "    @staticmethod\n",
    "    def _stack_goals(goal_tensors_all, num_nodes_all):\n",
    "        stacked_goal_embeds_all = []\n",
    "        for i in range(len(num_nodes_all)):\n",
    "            goal = goal_tensors_all[i]\n",
    "            num_nodes = num_nodes_all[i]\n",
    "            stacked_goal_embeds_all.append(torch.stack([goal] * num_nodes))\n",
    "        stacked_goal_emebds_all_tensor = torch.cat(stacked_goal_embeds_all, dim=0)\n",
    "        return stacked_goal_emebds_all_tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def propogate_multi(gnn, states, actions=None):\n",
    "        node_feats_tensor = torch.tensor(node_feats, device=device, requires_grad=True, dtype=torch.float)\n",
    "        node_states_all, goal_feats_all, num_nodes_all, predecessors_all, total_num_nodes = Environment._unpack_states(states, node_feats_tensor)\n",
    "        # If goal_input_layer is True then embed the goal by sending it into the input layer\n",
    "        if goal_C['goal_input_layer']:\n",
    "            goal_embeddings = gnn.input_model(goal_feats_all)\n",
    "            assert goal_embeddings.shape == (len(states), model_C['node_hidden_size'])\n",
    "            stacked_goal_embeds = Environment._stack_goals(goal_embeddings, num_nodes_all)\n",
    "            assert stacked_goal_embeds.shape == (total_num_nodes, model_C['node_hidden_size'])\n",
    "        else:\n",
    "            stacked_goal_embeds = Environment._stack_goals(goal_feats_all, num_nodes_all)\n",
    "            assert stacked_goal_embeds.shape == (total_num_nodes, model_C['node_feat_size'])\n",
    "                \n",
    "        for p in range(episode_C['num_props']):\n",
    "            node_states_all, prediction = gnn(node_states_all, p == 0, p == episode_C['num_props']-1, predecessors_all, stacked_goal_embeds, num_nodes_all, actions)\n",
    "#             assert node_states.shape == (num_nodes, model_C['node_hidden_size'])\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    agent = PPOAgent(Environment(), Environment())\n",
    "    \n",
    "    train_step = 0\n",
    "    rollout_times, batch_times, pred_times = [], [], []\n",
    "    for r in range(episode_C['num_train_rollouts']+1):\n",
    "#         print('ROLLOUT {}'.format(r))\n",
    "        if r != 0 and r % episode_C['eval_freq'] == 0:\n",
    "            print('At train step: {}  Rollout step time: {:.4f}  Batch time: {:.4f}  Pred time: {:.4f}'.format(train_step, np.array(rollout_times).mean(), np.array(batch_times).mean(), np.array(pred_times).mean()))\n",
    "            avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken = agent.eval_episodes()\n",
    "            print('Testing summary: Avg ep rew: {:.2f}  Max ep rew: {}  Min ep rew: {}  Achieved goal percent: {:.2f}  Avg opt steps: {:.2f}  Avg steps taken: {:.2f}\\n'.format(avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken))\n",
    "            rollout_times, batch_times, pred_times = [], [], []\n",
    "        train_step, rollout_step_time, train_batch_time, train_pred_time = agent.train_rollout(train_step)\n",
    "        rollout_times.append(rollout_step_time); batch_times.append(train_batch_time); pred_times.append(train_pred_time)\n",
    "        \n",
    "    # Run eval last time and record\n",
    "    avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken = agent.eval_episodes()\n",
    "    print('Final testing summary: Avg ep rew: {:.2f}  Max ep rew: {}  Min ep rew: {}  Achieved goal percent: {:.2f}  Avg opt steps: {:.2f}  Avg steps taken: {:.2f}\\n'.format(avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken))\n",
    "\n",
    "    # Return all info that is appended to the df of experiments (make a dict)\n",
    "    run_info = {}\n",
    "    def add_hyp_param_dict(append_letter, dic):\n",
    "        for k, v in list(dic.items()):\n",
    "            run_info[append_letter + '_' + k] = v\n",
    "    add_hyp_param_dict('E', episode_C)\n",
    "    add_hyp_param_dict('M', model_C)\n",
    "    add_hyp_param_dict('G', goal_C)\n",
    "    add_hyp_param_dict('A', agent_C)\n",
    "    add_hyp_param_dict('O', other_C)\n",
    "    run_info['eval_avg_ep_rew'] = avg_rew; run_info['eval_max_ep_rew'] = max_rew; run_info['eval_min_ep_rew'] = min_rew\n",
    "    run_info['eval_ach_goal_perc'] = ach_perc; run_info['eval_avg_opt_steps'] = avg_opt_steps\n",
    "    run_info['eval_avg_steps_taken'] = avg_steps_taken\n",
    "    return run_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_normal():\n",
    "    # Load constants\n",
    "    constants = load_constants('constants/PPO/constants.json')\n",
    "    \n",
    "    global episode_C; global model_C; global goal_C; global agent_C; global other_C\n",
    "    \n",
    "    episode_C, model_C, goal_C, agent_C, other_C = constants['episode_C'], constants['model_C'], constants['goal_C'], constants['agent_C'], constants['other_C']\n",
    "    # Fill in missing values\n",
    "    fill_in_missing_hyp_params(model_C, goal_C, len(pages), len(edges), node_feats.shape[1])\n",
    "    \n",
    "    exp_start = time.time()\n",
    "    _ = run()\n",
    "    exp_end = time.time()\n",
    "    print('Time taken (m): {:.2f}'.format((exp_end - exp_start) / 60.))\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_random_search(num_experiments, grid, df, columns):\n",
    "    for experiment in range(num_experiments):\n",
    "        exp_start = time.time()\n",
    "        \n",
    "        print('\\n --- Running experiment {} --- '.format(experiment))\n",
    "        \n",
    "        global episode_C; global model_C; global goal_C; global agent_C; global other_C\n",
    "        \n",
    "        # First pick the hyp params to use\n",
    "        episode_C, model_C, goal_C, agent_C, other_C = select_hyp_params(grid)\n",
    "        \n",
    "        fill_in_missing_hyp_params(model_C, goal_C, len(pages), len(edges), node_feats.shape[1])\n",
    "\n",
    "        run_info = run()\n",
    "#         assert len(run_info) == len(columns)\n",
    "        \n",
    "        # Add to df\n",
    "        df = df.append(run_info, ignore_index=True)\n",
    "        # Save fig\n",
    "        plt.savefig('train-grad-plots/plot_'+str(experiment)+'.png')\n",
    "        \n",
    "        exp_end = time.time()\n",
    "        print('Time taken (m): {:.2f}'.format((exp_end - exp_start) / 60.))\n",
    "        \n",
    "        df.to_excel('run-data.xlsx', index=False)\n",
    "        \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refresh_excel('run-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ad0bb668f016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-14470e3e903b>\u001b[0m in \u001b[0;36mrun_normal\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mexp_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mexp_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken (m): {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_end\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mexp_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-d3a1f8d39be4>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Testing summary: Avg ep rew: {:.2f}  Max ep rew: {}  Min ep rew: {}  Achieved goal percent: {:.2f}  Avg opt steps: {:.2f}  Avg steps taken: {:.2f}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_rew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_rew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_rew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mach_perc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_opt_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_steps_taken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mrollout_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrollout_step_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pred_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_rollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mrollout_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout_step_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mbatch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpred_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pred_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-55db3c5f979b>\u001b[0m in \u001b[0;36mtrain_rollout\u001b[1;34m(self, total_step)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0magent_C\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clip_grads'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_C\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient_clip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Max Brenner\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Max Brenner\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "run_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load grid of constants\n",
    "grid = load_constants('constants/PPO/constants-grid.json')\n",
    "# Load df for saving data\n",
    "df = pd.read_excel('run-data.xlsx')\n",
    "columns = df.columns.tolist()\n",
    "run_random_search(2, grid, df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
