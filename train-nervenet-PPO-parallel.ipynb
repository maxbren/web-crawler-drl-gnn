{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NerveNet_GNN'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d74d86b0c2d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnervenet_PPO\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNerveNet_GNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Max Brenner\\Desktop\\web-crawler-drl-gnn\\nervenet_PPO.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_grad_flow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_init_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Max Brenner\\Desktop\\web-crawler-drl-gnn\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnervenet_PPO\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNerveNet_GNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NerveNet_GNN'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict, deque, defaultdict\n",
    "from nervenet_PPO import NerveNet_GNN\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: when you see a loss of nan in the prints its okay, if the num of steps is less than the freq of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of things todo\n",
    "- ~~Add Double DQN and make sure vanilla DQN formula is correct~~\n",
    "- ~~Need to track the grads and detach the grads for outputs that arent a part of the actual minibatch fit~~\n",
    "- ~~Debug multiple props~~\n",
    "- ~~Track the grad for multiple props~~\n",
    "- ~~Go through \"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\"~~\n",
    "- ~~After going through ^ see if i need to change my loss because i think that I need to reshape or something so that the vector is summed and then meaned which means that the mean does nothing since there is only one sample per train anyway~~\n",
    "- ~~Also make sure to add huber loss~~\n",
    "- ~~Add dikstras to get optimal path from init to goal and use this both as reward and to limit the the path length in an ep~~\n",
    "- ~~check if adding cuda actually made it slower~~\n",
    "- ~~Getting a loss of nan sometimes after running it for a while, see whats up~~\n",
    "- ~~a framework/csv to keep track of all hyperparams when im tuning, even remember things that arent constants such as what feats i use for a node etc.~~\n",
    "   ~~- Might also want to add time data for certain parts of the code~~\n",
    "~~- add soft updates for target model~~\n",
    "- Try adding the link encodings to each node feature cuz right now i just use title\n",
    "- Try instead of looping through for each prop, expliclty unrolling each prop to make sure that replacing the var node states isnt an issue\n",
    "- See if i need node feats and goal feats to have grad\n",
    "- Add in ignore intenral nodes and see if that improves performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab data and make Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_whole, pages, node_feats, edges = load_data_make_graph('data/animals-D3-small-30K-nodes40-edges202-max10-minout2-minin3_w_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(G_whole, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, train_env, eval_env, shared_gnn, optimizer):\n",
    "        self.train_env = train_env\n",
    "        self.eval_env = eval_env\n",
    "        self.gnn = NerveNet_GNN(model_C['node_feat_size'], model_C['node_hidden_size'], \n",
    "                                model_C['message_size'], model_C['output_size'], \n",
    "                                goal_C['goal_size'], goal_C['goal_opt'], agent_C['critic_agg_weight'], \n",
    "                                device).to(device)\n",
    "        self.shared_gnn = shared_gnn\n",
    "        self.state = self.train_env.reset()\n",
    "        self.ep_step = 0\n",
    "#         self.opt = torch.optim.Adam(self.gnn.parameters(), agent_C['learning_rate'])\n",
    "        self.opt = optimizer\n",
    "        self.gnn.eval()\n",
    "    \n",
    "    def _eval_episode(self, test_step):\n",
    "        state = self.eval_env.reset()\n",
    "        shortest_path_length = state[4]\n",
    "        ep_rew = 0\n",
    "        for step in range(episode_C['max_ep_steps']):\n",
    "            prediction = self.eval_env.propogate_multi(self.gnn, [state])\n",
    "            action = prediction['a'].cpu().numpy()[0]\n",
    "            state = deepcopy(state)\n",
    "            next_state, reward, done, achieved_goal = self.eval_env.step(action, step, state)\n",
    "            if achieved_goal: assert done\n",
    "            ep_rew += reward\n",
    "            test_step += 1\n",
    "            if done:\n",
    "                break\n",
    "            state = deepcopy(next_state)\n",
    "        return test_step, ep_rew, achieved_goal, shortest_path_length-1, step+1\n",
    "        \n",
    "    def eval_episodes(self):\n",
    "        test_step = 0\n",
    "        test_info = {}\n",
    "        test_info['all ep rew'] = []\n",
    "        test_info['max ep rew'] = float('-inf')\n",
    "        test_info['min ep rew'] = float('inf')\n",
    "        test_info['achieved goal'] = []\n",
    "        test_info['opt steps'] = []\n",
    "        test_info['steps taken'] = []\n",
    "        for ep in range(episode_C['eval_num_eps']):\n",
    "            test_step, ep_rew, achieved_goal, opt_steps, steps_taken = self._eval_episode(test_step)\n",
    "            test_info['all ep rew'].append(ep_rew)\n",
    "            test_info['max ep rew'] = max(test_info['max ep rew'], ep_rew)\n",
    "            test_info['min ep rew'] = min(test_info['min ep rew'], ep_rew)\n",
    "            test_info['achieved goal'].append(achieved_goal)\n",
    "            test_info['opt steps'].append(opt_steps)\n",
    "            test_info['steps taken'].append(steps_taken)\n",
    "        return (np.array(test_info['max ep rew']).mean(),\n",
    "                test_info['max ep rew'],\n",
    "                test_info['min ep rew'],\n",
    "                np.array(test_info['achieved goal']).sum() / ep,\n",
    "                np.array(test_info['opt steps']).mean(),\n",
    "                np.array(test_info['steps taken']).mean())\n",
    "        \n",
    "    def train_rollout(self, total_step):\n",
    "        storage = Storage(episode_C['rollout_length'])\n",
    "        state = Environment._copy_state(*self.state)\n",
    "        step_times = []\n",
    "        # Sync.\n",
    "        self.gnn.load_state_dict(self.shared_gnn.state_dict())\n",
    "        for rollout_step in range(episode_C['rollout_length']):\n",
    "            start_step_time = time.time()\n",
    "            prediction = self.train_env.propogate_multi(self.gnn, [state])\n",
    "            action = prediction['a'].cpu().numpy()[0]\n",
    "            next_state, reward, done, achieved_goal = self.train_env.step(action, self.ep_step, state)\n",
    "\n",
    "            self.ep_step += 1\n",
    "            if done:\n",
    "                # Sync local model with shared model at start of each ep\n",
    "                self.gnn.load_state_dict(self.shared_gnn.state_dict())\n",
    "                self.ep_step = 0\n",
    "                \n",
    "            storage.add(prediction)\n",
    "            storage.add({'r': tensor(reward, device).unsqueeze(-1).unsqueeze(-1),\n",
    "                         'm': tensor(1 - done, device).unsqueeze(-1).unsqueeze(-1),\n",
    "                         's': state})\n",
    "\n",
    "            state = Environment._copy_state(*next_state)\n",
    "            \n",
    "            total_step += 1\n",
    "            \n",
    "            end_step_time = time.time()\n",
    "            step_times.append(end_step_time - start_step_time)\n",
    "            \n",
    "        self.state = Environment._copy_state(*state)\n",
    "\n",
    "        prediction = self.train_env.propogate_multi(self.gnn, [state])\n",
    "        storage.add(prediction)\n",
    "        storage.placeholder()\n",
    "        \n",
    "        advantages = tensor(np.zeros((1, 1)), device)\n",
    "        returns = prediction['v'].detach()\n",
    "        for i in reversed(range(episode_C['rollout_length'])):\n",
    "            # Disc. Return\n",
    "            returns = storage.r[i] + agent_C['discount'] * storage.m[i] * returns\n",
    "            # GAE\n",
    "            td_error = storage.r[i] + agent_C['discount'] * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "            advantages = advantages * agent_C['gae_tau'] * agent_C['discount'] * storage.m[i] + td_error\n",
    "            storage.adv[i] = advantages.detach()\n",
    "            storage.ret[i] = returns.detach()\n",
    "        \n",
    "#         print(returns.shape, td_error.shape, advantages.shape, storage.adv[-1].shape, storage.ret[-1].shape)\n",
    "        \n",
    "        actions, log_probs_old, returns, advantages = storage.cat(['a', 'log_pi_a', 'ret', 'adv'])\n",
    "        states = [storage.s[i] for i in range(storage.size)]\n",
    "        \n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        # Train\n",
    "        self.gnn.train()\n",
    "        batch_times = []\n",
    "        train_pred_times = []\n",
    "        for _ in range(agent_C['optimization_epochs']):\n",
    "            # Sync. at start of each epoch TODO: TEST IF THIS IS OKAY!!\n",
    "            self.gnn.load_state_dict(self.shared_gnn.state_dict())\n",
    "            sampler = random_sample(np.arange(len(states)), agent_C['minibatch_size'])\n",
    "            for batch_indices in sampler:\n",
    "                start_batch_time = time.time()\n",
    "                \n",
    "                batch_indices_tensor = tensor(batch_indices, device).long()\n",
    "\n",
    "                # Important Node: these are tensors but dont have a grad\n",
    "                sampled_states = [states[i] for i in batch_indices]\n",
    "                sampled_actions = actions[batch_indices_tensor]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices_tensor]\n",
    "                sampled_returns = returns[batch_indices_tensor]\n",
    "                sampled_advantages = advantages[batch_indices_tensor]\n",
    "                \n",
    "                start_pred_time = time.time()\n",
    "                prediction = self.train_env.propogate_multi(self.gnn, sampled_states, sampled_actions)\n",
    "                end_pred_time = time.time()\n",
    "                train_pred_times.append(end_pred_time - start_pred_time)\n",
    "                \n",
    "                # Calc. Loss\n",
    "#                 self.gnn.train()\n",
    "                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - agent_C['ppo_ratio_clip'],\n",
    "                                          1.0 + agent_C['ppo_ratio_clip']) * sampled_advantages\n",
    "\n",
    "                # policy loss and value loss are scalars\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean() - agent_C['entropy_weight'] * prediction['ent'].mean()\n",
    "\n",
    "                value_loss = agent_C['value_loss_coef'] * (sampled_returns - prediction['v']).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                if agent_C['clip_grads']:\n",
    "                    nn.utils.clip_grad_norm_(self.gnn.parameters(), agent_C['gradient_clip'])\n",
    "                ensure_shared_grads(self.gnn, self.shared_gnn)\n",
    "#                 self.gnn.graph_grads()\n",
    "                self.opt.step()\n",
    "#                 self.gnn.eval()\n",
    "                end_batch_time = time.time()\n",
    "                batch_times.append(end_batch_time - start_batch_time)\n",
    "        self.gnn.eval()\n",
    "        return total_step, np.array(step_times).mean(), np.array(batch_times).mean(), np.array(train_pred_times).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Right now its just a collection of static methods, but I might wanna make it an actually useful class and\n",
    "# give it internal memory\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def step(selected_node_rel, ep_step, state):\n",
    "        state_copy = Environment._copy_state(*state)\n",
    "        G_curr, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors = state_copy[:]\n",
    "        \n",
    "        selected_node_abs = list(current_nodes.keys())[selected_node_rel]\n",
    "        achieved_goal = Environment.add_children(G_curr, selected_node_abs, goal_node, current_nodes)\n",
    "        # Done if goal found or end of ep\n",
    "        done = True if (ep_step == episode_C['max_ep_steps'] - 1) or achieved_goal else False\n",
    "        reward = Environment.reward_func(done, achieved_goal, shortest_path_length, ep_step+1)\n",
    "        predecessors = Environment.get_predecessors(G_curr, current_nodes)\n",
    "        \n",
    "        if not done:\n",
    "            next_state = Environment._copy_state(G_curr, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors)\n",
    "        else:\n",
    "            next_state = Environment.reset()\n",
    "        \n",
    "        return next_state, reward, done, achieved_goal\n",
    "    \n",
    "    # Deep copy everything before it goes into a state\n",
    "    @staticmethod\n",
    "    def _copy_state(G_curr, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors):\n",
    "        G_curr_copy = G_curr.copy()\n",
    "        current_nodes_copy = current_nodes.copy()\n",
    "        goal_feats_copy = goal_feats.copy()\n",
    "        predecessors_copy = deepcopy(predecessors)\n",
    "        \n",
    "        return (G_curr_copy, current_nodes_copy, goal_node, goal_feats_copy, shortest_path_length, predecessors_copy)\n",
    "    \n",
    "    # For now this is -1 per timestep +5 on terminal for reaching goal, -5 on terminal for not reaching goal\n",
    "    # And when it reaches the goal, give add (shortest_path_length - 1) - num actions taken (neg number)\n",
    "    @staticmethod\n",
    "    def reward_func(terminal, reach_goal, shortest_path_length, num_actions_taken):\n",
    "        rew = -1\n",
    "        if terminal:\n",
    "            if reach_goal:\n",
    "                assert num_actions_taken >= (shortest_path_length-1)\n",
    "                rew += other_C['reach_goal_rew']\n",
    "                rew += ((shortest_path_length-1) - num_actions_taken)  # optimal num actions - num actions taken\n",
    "            else:\n",
    "                rew += other_C['not_reach_goal_rew']\n",
    "        return rew\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        current_try = 0\n",
    "        while True:\n",
    "            current_try += 1\n",
    "    #         if current_try >= 50:\n",
    "    #              print('Current try for initialize ep is at: {}'.format(current_try))\n",
    "            init_node = random.randint(0, model_C['num_nodes']-1)\n",
    "            goal_node = random.randint(0, model_C['num_nodes']-1)\n",
    "            # restart if goal node is init node, or no path\n",
    "            if init_node == goal_node or not nx.has_path(G_whole, init_node, goal_node):\n",
    "                continue\n",
    "            # restart if shortest path is too long or too short\n",
    "            shortest_path_length = nx.shortest_path_length(G_whole, init_node, goal_node)\n",
    "            if shortest_path_length < episode_C['shortest_path_range_allowed_MIN'] or shortest_path_length > episode_C['shortest_path_range_allowed_MAX']:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # Get goal feats\n",
    "        goal_feats = node_feats[goal_node]\n",
    "        assert goal_feats.shape == (model_C['node_feat_size'],)\n",
    "        # Make init graph\n",
    "        G_init = nx.DiGraph()\n",
    "        G_init.add_node(init_node)\n",
    "        current_nodes = OrderedDict({init_node: 0})  # Init current nodes dict\n",
    "        got_goal = Environment.add_children(G_init, init_node, goal_node, current_nodes)\n",
    "        assert sorted(list(current_nodes.values())) == list(current_nodes.values())\n",
    "        assert not got_goal\n",
    "        predecessors = Environment.get_predecessors(G_init, current_nodes)\n",
    "        return (G_init, current_nodes, goal_node, goal_feats, shortest_path_length, predecessors)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_children(G_curr, node_indx, goal_node_indx, current_nodes):\n",
    "        achieved_goal = False\n",
    "        # Check the children of the node to see if they need to be added to the current graph\n",
    "        children = G_whole.successors(node_indx)\n",
    "        for child in children:\n",
    "            # Add child if not in G and check if goal\n",
    "            if child not in G_curr:\n",
    "                G_curr.add_node(child)\n",
    "                current_nodes.update({child: len(current_nodes)})\n",
    "                if child == goal_node_indx:\n",
    "                    achieved_goal = True\n",
    "            # If the edge doesnt exist add it\n",
    "            if not G_curr.has_edge(node_indx, child):\n",
    "                G_curr.add_edge(node_indx, child)\n",
    "        assert sorted(list(current_nodes.values())) == list(current_nodes.values())\n",
    "        return achieved_goal\n",
    "    \n",
    "    # current_nodes: ordereddict with keys as abs node indices, values as rel node indices (rel to the ordered dict)\n",
    "    @staticmethod\n",
    "    def get_predecessors(G_curr, current_nodes):\n",
    "        all_preds = []  # List of lists\n",
    "        for node in current_nodes.keys():\n",
    "            preds_abs = G_curr.predecessors(node)  # abs to all nodes, keys to the dict\n",
    "            preds_rel = [current_nodes[x] for x in preds_abs]\n",
    "            all_preds.append(preds_rel)\n",
    "        return all_preds  # Returns a list of lists with the values being tth rel node indices\n",
    "    \n",
    "    @staticmethod\n",
    "    def _unpack_states(states, node_feats_tensor):\n",
    "        num_nodes_all = []\n",
    "        goal_feats_all = []\n",
    "        predecessors_all = []\n",
    "        node_states_all = []\n",
    "        total_num_nodes = 0\n",
    "        for state in states:\n",
    "            G_curr, current_nodes, goal_node, goal_feats, shortest_path, predecessors = state[:]\n",
    "            num_nodes_all.append(len(current_nodes))\n",
    "            total_num_nodes += num_nodes_all[-1]\n",
    "            goal_feats_tensor = torch.tensor(goal_feats, device=device, requires_grad=True, dtype=torch.float)\n",
    "            goal_feats_all.append(goal_feats_tensor)\n",
    "            predecessors_all.append(predecessors)\n",
    "            node_states_all.append(node_feats_tensor[list(current_nodes.keys())])\n",
    "        node_states_all_tensor = torch.cat(node_states_all, dim=0)\n",
    "        goal_feats_all_tensor = torch.stack(goal_feats_all, dim=0)\n",
    "\n",
    "#         print(node_states_all_tensor.shape, goal_feats_all_tensor.shape, len(num_nodes_all), len(predecessors_all), total_num_nodes)\n",
    "#         print(node_states_all_tensor, goal_feats_all_tensor, num_nodes_all, predecessors_all)\n",
    "    \n",
    "        return node_states_all_tensor, goal_feats_all_tensor, num_nodes_all, predecessors_all, total_num_nodes\n",
    "    \n",
    "    @staticmethod\n",
    "    def _stack_goals(goal_tensors_all, num_nodes_all):\n",
    "        stacked_goal_embeds_all = []\n",
    "        for i in range(len(num_nodes_all)):\n",
    "            goal = goal_tensors_all[i]\n",
    "            num_nodes = num_nodes_all[i]\n",
    "            stacked_goal_embeds_all.append(torch.stack([goal] * num_nodes))\n",
    "        stacked_goal_emebds_all_tensor = torch.cat(stacked_goal_embeds_all, dim=0)\n",
    "        return stacked_goal_emebds_all_tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def propogate_multi(gnn, states, actions=None):\n",
    "        node_feats_tensor = torch.tensor(node_feats, device=device, requires_grad=True, dtype=torch.float)\n",
    "        node_states_all, goal_feats_all, num_nodes_all, predecessors_all, total_num_nodes = Environment._unpack_states(states, node_feats_tensor)\n",
    "        # If goal_input_layer is True then embed the goal by sending it into the input layer\n",
    "        if goal_C['goal_input_layer']:\n",
    "            goal_embeddings = gnn.input_model(goal_feats_all)\n",
    "            assert goal_embeddings.shape == (len(states), model_C['node_hidden_size'])\n",
    "            stacked_goal_embeds = Environment._stack_goals(goal_embeddings, num_nodes_all)\n",
    "            assert stacked_goal_embeds.shape == (total_num_nodes, model_C['node_hidden_size'])\n",
    "        else:\n",
    "            stacked_goal_embeds = Environment._stack_goals(goal_feats_all, num_nodes_all)\n",
    "            assert stacked_goal_embeds.shape == (total_num_nodes, model_C['node_feat_size'])\n",
    "                \n",
    "        for p in range(episode_C['num_props']):\n",
    "            node_states_all, prediction = gnn(node_states_all, p == 0, p == episode_C['num_props']-1, predecessors_all, stacked_goal_embeds, num_nodes_all, actions)\n",
    "#             assert node_states.shape == (num_nodes, model_C['node_hidden_size'])\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def run():\n",
    "#     agent = PPOAgent(Environment(), Environment())\n",
    "    \n",
    "#     train_step = 0\n",
    "#     rollout_times, batch_times, pred_times = [], [], []\n",
    "#     for r in range(episode_C['num_train_rollouts']+1):\n",
    "# #         print('ROLLOUT {}'.format(r))\n",
    "#         if r != 0 and r % episode_C['eval_freq'] == 0:\n",
    "# #             print('At train step: {}  Rollout step time: {:.4f}  Batch time: {:.4f}  Pred time: {:.4f}'.format(train_step, np.array(rollout_times).mean(), np.array(batch_times).mean(), np.array(pred_times).mean()))\n",
    "#             avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken = agent.eval_episodes()\n",
    "# #             print('Testing summary: Avg ep rew: {:.2f}  Max ep rew: {}  Min ep rew: {}  Achieved goal percent: {:.2f}  Avg opt steps: {:.2f}  Avg steps taken: {:.2f}\\n'.format(avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken))\n",
    "#             rollout_times, batch_times, pred_times = [], [], []\n",
    "#         train_step, rollout_step_time, train_batch_time, train_pred_time = agent.train_rollout(train_step)\n",
    "#         rollout_times.append(rollout_step_time); batch_times.append(train_batch_time); pred_times.append(train_pred_time)\n",
    "        \n",
    "#     # Run eval last time and record\n",
    "#     avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken = agent.eval_episodes()\n",
    "#     print('Final testing summary: Avg ep rew: {:.2f}  Max ep rew: {}  Min ep rew: {}  Achieved goal percent: {:.2f}  Avg opt steps: {:.2f}  Avg steps taken: {:.2f}\\n'.format(avg_rew, max_rew, min_rew, ach_perc, avg_opt_steps, avg_steps_taken))\n",
    "\n",
    "#     # Return all info that is appended to the df of experiments (make a dict)\n",
    "#     run_info = {}\n",
    "#     def add_hyp_param_dict(append_letter, dic):\n",
    "#         for k, v in list(dic.items()):\n",
    "#             run_info[append_letter + '_' + k] = v\n",
    "#     add_hyp_param_dict('E', episode_C)\n",
    "#     add_hyp_param_dict('M', model_C)\n",
    "#     add_hyp_param_dict('G', goal_C)\n",
    "#     add_hyp_param_dict('A', agent_C)\n",
    "#     add_hyp_param_dict('O', other_C)\n",
    "#     run_info['eval_avg_ep_rew'] = avg_rew; run_info['eval_max_ep_rew'] = max_rew; run_info['eval_min_ep_rew'] = min_rew\n",
    "#     run_info['eval_ach_goal_perc'] = ach_perc; run_info['eval_avg_opt_steps'] = avg_opt_steps\n",
    "#     run_info['eval_avg_steps_taken'] = avg_steps_taken\n",
    "#     return run_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target for multiprocess\n",
    "def train(shared_gnn, optimizer, rollout_counter):\n",
    "    agent = PPOAgent(Environment(), Environment(), shared_gnn, optimizer)\n",
    "    train_step = 0\n",
    "    rollout_times, batch_times, pred_times = [], [], []\n",
    "#     for r in range(episode_C['num_train_rollouts']+1):\n",
    "    while True:\n",
    "        agent.train_rollout(train_step)\n",
    "        rollout_counter.increment()\n",
    "        if rollout_counter.get() >= episode_C['num_train_rollouts']+1:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    shared_gnn = NerveNet_GNN(model_C['node_feat_size'], model_C['node_hidden_size'], \n",
    "                                model_C['message_size'], model_C['output_size'], \n",
    "                                goal_C['goal_size'], goal_C['goal_opt'], agent_C['critic_agg_weight'], \n",
    "                                device).to(device)\n",
    "    shared_gnn.share_memory()\n",
    "    optimizer = torch.optim.Adam(shared_gnn.parameters(), agent_C['learning_rate'])\n",
    "#     optimizer.share_memory()\n",
    "    rollout_counter = Counter()  # To keep track of all the rollouts amongst agents\n",
    "    processes = []\n",
    "    NUM_AGENTS = 1\n",
    "    for _ in range(NUM_AGENTS):\n",
    "        p = mp.Process(target=train, args=(shared_gnn, optimizer, rollout_counter))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    # Kill ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_normal():\n",
    "    # Load constants\n",
    "    constants = load_constants('constants/PPO/constants.json')\n",
    "    \n",
    "    global episode_C; global model_C; global goal_C; global agent_C; global other_C\n",
    "    \n",
    "    episode_C, model_C, goal_C, agent_C, other_C = constants['episode_C'], constants['model_C'], constants['goal_C'], constants['agent_C'], constants['other_C']\n",
    "    # Fill in missing values\n",
    "    fill_in_missing_hyp_params(model_C, goal_C, len(pages), len(edges), node_feats.shape[1])\n",
    "    \n",
    "    exp_start = time.time()\n",
    "#     _ = run()\n",
    "    run()\n",
    "    exp_end = time.time()\n",
    "    print('Time taken (m): {:.2f}'.format((exp_end - exp_start) / 60.))\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_random_search(num_experiments, grid, df, columns):\n",
    "    for experiment in range(num_experiments):\n",
    "        exp_start = time.time()\n",
    "        \n",
    "        print('\\n --- Running experiment {} --- '.format(experiment))\n",
    "        \n",
    "        global episode_C; global model_C; global goal_C; global agent_C; global other_C\n",
    "        \n",
    "        # First pick the hyp params to use\n",
    "        episode_C, model_C, goal_C, agent_C, other_C = select_hyp_params(grid)\n",
    "        \n",
    "        fill_in_missing_hyp_params(model_C, goal_C, len(pages), len(edges), node_feats.shape[1])\n",
    "\n",
    "        run_info = run()\n",
    "#         assert len(run_info) == len(columns)\n",
    "        \n",
    "        # Add to df\n",
    "        df = df.append(run_info, ignore_index=True)\n",
    "        # Save fig\n",
    "        plt.savefig('train-grad-plots/plot_'+str(experiment)+'.png')\n",
    "        \n",
    "        exp_end = time.time()\n",
    "        print('Time taken (m): {:.2f}'.format((exp_end - exp_start) / 60.))\n",
    "        \n",
    "        df.to_excel('run-data.xlsx', index=False)\n",
    "        \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refresh_excel('run-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Load grid of constants\n",
    "# grid = load_constants('constants/PPO/constants-grid.json')\n",
    "# # Load df for saving data\n",
    "# df = pd.read_excel('run-data.xlsx')\n",
    "# columns = df.columns.tolist()\n",
    "# run_random_search(2, grid, df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
